import numpy as np
import onnx
import os
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy
import argparse

def generate_calibration_data(input_shape, num_samples=100):
    """ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìš© ë”ë¯¸ ë°ì´í„° ìƒì„±"""
    return np.random.random(size=(num_samples, *input_shape[1:])).astype(np.float32)


class Calibrator(trt.IInt8Calibrator):
    def __init__(self, calibration_data, batch_size, input_shape, cache_file):
        super().__init__()
        self.calibration_data = calibration_data
        self.batch_size = batch_size
        self.current_index = 0
        self.input_shape = input_shape
        self.cache_file = cache_file

        # ë©”ëª¨ë¦¬ í¬ê¸° ê³„ì‚°ì„ int64ë¡œ í™•ì‹¤íˆ ì²˜ë¦¬
        size_in_bytes = int(self.batch_size * numpy.prod(input_shape) * numpy.dtype(numpy.float32).itemsize)

        # GPU ë©”ëª¨ë¦¬ í• ë‹¹
        self.device_input = cuda.mem_alloc(size_in_bytes)

    def get_batch_size(self):
        return self.batch_size

    def get_batch(self, names):
        if self.current_index + self.batch_size > len(self.calibration_data):
            return None

        batch = self.calibration_data[self.current_index:self.current_index + self.batch_size]
        cuda.memcpy_htod(self.device_input, batch.astype(np.float32))
        self.current_index += self.batch_size
        return [self.device_input]

    def read_calibration_cache(self):
        if os.path.exists(self.cache_file):
            with open(self.cache_file, "rb") as f:
                return f.read()
        return None

    def write_calibration_cache(self, cache):
        with open(self.cache_file, "wb") as f:
            f.write(cache)

    def get_algorithm(self):
        return trt.CalibrationAlgoType.ENTROPY_CALIBRATION_2  # ë˜ëŠ” ENTROPY_CALIBRATION, LEGACY_CALIBRATION


def build_engine_fp16(onnx_path, engine_path, input_tensor_name, input_shape=(1, 3, 60, 60)):
    """FP16 TensorRT ì—”ì§„ ìƒì„±"""
    if not os.path.exists(onnx_path):
        raise FileNotFoundError(f"âŒ ONNX íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {onnx_path}")

    TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)

    with trt.Builder(TRT_LOGGER) as builder, \
            builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \
            trt.OnnxParser(network, TRT_LOGGER) as parser:

        config = builder.create_builder_config()
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 32)

        # FP16ë§Œ ì„¤ì •
        config.flags = 1 << int(trt.BuilderFlag.FP16)

        # ì…ë ¥ í…ì„œ ì„¤ì •
        print(f"ì…ë ¥ í…ì„œ í¬ê¸° ì„¤ì •: {input_shape}")
        profile = builder.create_optimization_profile()
        profile.set_shape(input_tensor_name, input_shape, input_shape, input_shape)
        config.add_optimization_profile(profile)

        # ONNX íŒŒì¼ íŒŒì‹± ë° ì—”ì§„ ë¹Œë“œ
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                print('ONNX íŒŒì¼ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
                for error_index in range(parser.num_errors):
                    print(f"íŒŒì„œ ì—ëŸ¬ [{error_index}]: {parser.get_error(error_index)}")
                raise ValueError("ONNX íŒŒì¼ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        try:
            print('FP16 TensorRT ì—”ì§„ ë¹Œë“œ ì¤‘...')
            serialized_engine = builder.build_serialized_network(network, config)
            if serialized_engine:
                print(f'FP16 ì—”ì§„ ë¹Œë“œ ì„±ê³µ! ì €ì¥ ì¤‘: {engine_path}')
                with open(engine_path, 'wb') as f:
                    f.write(serialized_engine)
            else:
                print('FP16 ì—”ì§„ ë¹Œë“œ ì‹¤íŒ¨!')
        except Exception as e:
            print(f'FP16 ì—”ì§„ ë¹Œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}')
            serialized_engine = None

        return serialized_engine



def build_engine_int8(onnx_path, engine_path, input_tensor_name, input_shape=(1, 3, 60, 60)):
    if not os.path.exists(onnx_path):
        raise FileNotFoundError(f"âŒ ONNX íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {onnx_path}")

    # Calibration ì„¤ì •
    batch_size = 1
    cache_file = os.path.join(os.path.dirname(engine_path), "calibration.cache")
    calibration_data = generate_calibration_data(input_shape, num_samples=100)

    TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)

    with trt.Builder(TRT_LOGGER) as builder, \
            builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \
            trt.OnnxParser(network, TRT_LOGGER) as parser:

        config = builder.create_builder_config()
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 32)

        # INT8 ì–‘ìí™” ì„¤ì •
        config.flags = (
                1 << int(trt.BuilderFlag.INT8) |
                1 << int(trt.BuilderFlag.FP16)
        )

        # Calibrator ì„¤ì •
        calibrator = Calibrator(
            calibration_data=calibration_data,
            batch_size=batch_size,
            input_shape=input_shape,
            cache_file=cache_file
        )
        config.int8_calibrator = calibrator

        # ì…ë ¥ í…ì„œ ì„¤ì •
        print(f"ì…ë ¥ í…ì„œ í¬ê¸° ì„¤ì •: {input_shape}")
        profile = builder.create_optimization_profile()
        profile.set_shape(input_tensor_name, input_shape, input_shape, input_shape)
        config.add_optimization_profile(profile)

        # ONNX íŒŒì¼ íŒŒì‹±
        print(f"ONNX íŒŒì¼ì„ ì½ëŠ” ì¤‘... {onnx_path}")
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                print('ONNX íŒŒì¼ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
                for error_index in range(parser.num_errors):
                    print(f"íŒŒì„œ ì—ëŸ¬ [{error_index}]: {parser.get_error(error_index)}")
                raise ValueError("ONNX íŒŒì¼ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # ì—”ì§„ ë¹Œë“œ
        try:
            print('TensorRT ì—”ì§„ ë¹Œë“œ ì¤‘...')
            serialized_engine = builder.build_serialized_network(network, config)
            if serialized_engine:
                print(f'ì—”ì§„ ë¹Œë“œ ì„±ê³µ! ì—”ì§„ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤: {engine_path}')
                with open(engine_path, 'wb') as f:
                    f.write(serialized_engine)
            else:
                print('ì—”ì§„ ë¹Œë“œ ì‹¤íŒ¨!')
        except Exception as e:
            print(f'ì—”ì§„ ë¹Œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}')
            serialized_engine = None

        return serialized_engine


def argument_parser():
    parser = argparse.ArgumentParser(description='TensorRT Engine Builder')
    parser.add_argument('--onnx_path', type=str, required=True, help='Path to the ONNX model file')
    parser.add_argument('--engine_path_fp16', type=str, required=True, help='Path to save the FP16 engine file')
    parser.add_argument('--engine_path_int8', type=str, required=True, help='Path to save the INT8 engine file')
    return parser.parse_args()

if __name__ == "__main__":


    # onnx_path = '~/Documents/A4VAI-SITL/ROS2/ros2_ws/src/pathplanning/pathplanning/model/weight.onnx'
    # engine_path_fp16 = onnx_path + "_fp16.trt"
    # engine_path_int8 = onnx_path + "_int8.trt"

    args = argument_parser()
    onnx_path = args.onnx_path
    engine_path_fp16 = args.engine_path_fp16
    engine_path_int8 = args.engine_path_int8

    input_tensor_name = "observation"
    input_shape = (1, 3, 60, 60)

    # ONNX ëª¨ë¸ ì²´í¬
    try:
        model = onnx.load(onnx_path)
        onnx.checker.check_model(model)
        print("âœ… ONNX íŒŒì¼ì´ ì •ìƒì…ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ONNX íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤: {e}")

    # FP16 ì—”ì§„ ìƒì„±
    print("\nğŸ”„ FP16 ì—”ì§„ ìƒì„± ì¤‘...")
    engine_fp16 = build_engine_fp16(onnx_path, engine_path_fp16, input_tensor_name, input_shape)

    # INT8 ì—”ì§„ ìƒì„±
    print("\nğŸ”„ INT8 ì—”ì§„ ìƒì„± ì¤‘...")
    engine_int8 = build_engine_int8(onnx_path, engine_path_int8, input_tensor_name, input_shape)
